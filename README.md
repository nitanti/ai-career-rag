# ai-career-rag

An AI-powered career assistant that uses Retrieval-Augmented Generation (RAG) with Groq's LLaMA3 and FAISS to provide personalised career guidance.  
Users can upload their resume and ask career-related questions, with responses grounded in their own experience.

GitHub Project URL: https://github.com/nitanti/ai-career-rag

---

## âœ¨ Features

- RAG pipeline using LangChain, HuggingFace, FAISS
- Resume upload and PDF parsing
- LLM-powered Q&A with Groq LLaMA3
- Local document vector store using FAISS
- Docker-based deployment
- Railway CLI support for fast cloud deploy

---

## ğŸ§  Tech Stack

- FastAPI + Uvicorn
- LangChain + Groq (LLaMA3)
- HuggingFace Embeddings
- FAISS Vector Store
- Next.js + React + Tailwind CSS
- Railway + Docker

---

## ğŸ§ª Local Backend Development

### 1. Clone the repo

```bash
git clone https://github.com/nitanti/ai-career-rag.git
cd ai-career-rag/backend
```

### 2. (Optional) Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate.bat  # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables

Create a `.env` file in the `/backend` directory with:

```env
GROQ_API_KEY=your_groq_api_key
```

> ğŸ”‘ You can get your API key from: https://console.groq.com/keys

### 5. Run the backend app

```bash
uvicorn main:app --reload
```

> Backend will be available at: http://localhost:8000

---

## ğŸ³ Deploy Backend with Docker + Railway

### 1. Install Railway CLI (if not installed)

```bash
npm install -g @railway/cli
# or
brew install railway  # for Homebrew users
```

### 2. Log in and initialise project

```bash
railway login
railway init
```

### 3. Deploy backend from `/backend`

```bash
cd backend
railway up
```

> âš ï¸ Make sure `.env` exists and `.dockerignore` excludes heavy files like `venv/`, `__pycache__/`, `.ipynb_checkpoints/`, and large models to prevent build failures.

---

## ğŸ§© Frontend (Next.js)

The frontend is built with Next.js and provides:

- ğŸ“¤ Resume upload (.PDF, .DOCX, .TXT, .JPG, .JPEG, .PNG)
- ğŸ’¬ Question input for career queries
- ğŸ§  Real-time answer from backend LLM
- â³ Progress bar and loading UI

---

## âš™ï¸ Setup Frontend Locally

```bash
cd frontend
npm install
npm run dev
```

> App runs at: http://localhost:3000  
> Make sure backend is running on http://localhost:8000  
> If backend URL is different, update it in frontend config.

---

## ğŸ”— Frontendâ€“Backend Integration

### API Endpoints

1. `POST /upload` â€“ Uploads and parses resume documents, stores embeddings in FAISS.  
2. `POST /ask` â€“ Sends a career-related question and gets a response from Groq LLaMA3 based on uploaded data.

> The frontend uses `NEXT_PUBLIC_API_URL` to connect to backend.

---

## ğŸš€ Frontend Deployment (optional)

Deploy frontend separately via:

- Vercel  
- Netlify  
- Or Railway (if monorepo setup is used)

### Example `.env.local`:

```env
NEXT_PUBLIC_API_URL=https://your-backend-url.up.railway.app
```

---

## ğŸ“ Project Structure

```bash
ai-career-rag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ app/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md
```

---

## ğŸ“Œ Notes

- This project is under active development.
- Backend is powered by FastAPI, LangChain, Groq LLaMA3.
- Docker image is optimised using `.dockerignore`. Keep it clean to avoid exceeding build size.

---

## ğŸ‘©â€ğŸ’» Author

**Nichella Annarisa Tanti** (Nichanan Tantiwatanapaisal)  
[GitHub](https://github.com/nitanti) | [LinkedIn](https://www.linkedin.com/in/nichellatanti/)